{"cells":[{"cell_type":"code","execution_count":null,"id":"55c17572","metadata":{"id":"55c17572"},"outputs":[],"source":["import argparse\n","from datetime import datetime\n","import glob\n","import json\n","import os\n","import random\n","import sys\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import KFold\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch import optim\n","from torch.optim.lr_scheduler import StepLR\n","from torch.utils import data\n","from torchvision import models\n","from vgg import VGG8"]},{"cell_type":"code","execution_count":null,"id":"26fe221e","metadata":{"id":"26fe221e"},"outputs":[],"source":["import nibabel\n","import numpy as np\n","import pandas as pd\n","from scipy.ndimage import convolve1d\n","from torch.utils import data\n","\n","from utils import get_lds_kernel_window\n","\n","def prepare_weights(df, reweight, lds, lds_kernel, lds_ks, lds_sigma):\n","    if reweight == 'none':\n","        return None\n","\n","    bin_counts = df['iqbin'].value_counts()\n","    # num_per_label[i] = the number of subjects in the age bin of the ith subject in the dataset\n","    if reweight == 'inv':\n","        num_per_label = [bin_counts[bin] for bin in df['iqbin']]\n","    elif reweight == 'sqrt_inv':\n","        num_per_label = [np.sqrt(bin_counts[bin]) for bin in df['iqbin']]\n","\n","    if lds:\n","        lds_kernel_window = get_lds_kernel_window(lds_kernel, lds_ks, lds_sigma)\n","        smoothed_value = pd.Series(\n","            convolve1d(bin_counts.values, weights=lds_kernel_window, mode='constant'),\n","            index=bin_counts.index)\n","        num_per_label = [smoothed_value[bin] for bin in df['iqbin']]\n","\n","    weights = [1. / x for x in num_per_label]\n","    scaling = len(weights) / np.sum(weights)\n","    weights = [scaling * x for x in weights]\n","    return weights\n","\n","class AgePredictionDataset(data.Dataset):\n","    def __init__(self, df, iq, iq_type, n_slices, im_type, reweight='none', lds=False, lds_kernel='gaussian', lds_ks=9, lds_sigma=1, labeled=True):\n","        self.df = df\n","        self.iq = iq\n","        self.iq_type = iq_type\n","        self.n_slices = n_slices # my edits\n","        self.im_type = im_type # my edits\n","        self.weights = prepare_weights(df, reweight, lds, lds_kernel, lds_ks, lds_sigma)\n","        self.labeled = labeled\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        if self.im_type == \"int_rav\": # my edits\n","            image = nibabel.load(row['path']).get_fdata() # my edits\n","            row['path'] = row['path'].replace('/reg_', '/ravens_') # my edits\n","            ravens = nibabel.load(row['path']).get_fdata() # my edits\n","            image = image + ravens # my edits\n","        else:\n","            image = nibabel.load(row['path']).get_fdata()\n","\n","        #image = image[54:184, 25:195, 12:132] # Crop out zeroes --> original\n","        #st = 120 - (self.n_slices//2)\n","        #st = 150 - (self.n_slices//2)\n","        #ed = st + self.n_slices\n","        #image = image[st:ed, 25:195, 12:132] # # my edits; center 120th slice\n","        #image = image[50:190, 25:195, st:ed] #\n","        #image = np.transpose(image, (2, 0, 1))\n","\n","        st = 75 - (self.n_slices//2) #new edit\n","        ed = st + self.n_slices #new edit\n","        image = image[50:190, 25:195, st:ed] #new edit\n","        image = np.transpose(image, (2, 0, 1)) #new edit\n","\n","        image /= np.percentile(image, 95) # Normalize intensity\n","\n","        if self.labeled:\n","            if self.iq_type == 'absolute':\n","                if self.iq == 'all':\n","                    iq = np.array([row['fiq'], row['viq'], row['piq']]) # my edits\n","                elif self.iq == 'fiq':\n","                    iq = np.array([row['fiq']])\n","                elif self.iq == 'piq':\n","                    iq = np.array([row['piq']])\n","                elif self.iq == 'viq':\n","                    iq = np.array([row['viq']])\n","            else:\n","                if self.iq == 'all':\n","                    iq = np.array([row['fiq_r'], row['viq_r'], row['piq_r']]) # my edits\n","                elif self.iq == 'fiq':\n","                    iq = np.array([row['fiq_r']])\n","                elif self.iq == 'piq':\n","                    iq = np.array([row['piq_r']])\n","                elif self.iq == 'viq':\n","                    iq = np.array([row['viq_r']])\n","            weight = self.weights[idx] if self.weights is not None else 1.\n","            return (image, iq, weight)\n","        else:\n","            return image\n","\n","    def __len__(self):\n","        return len(self.df)\n"]},{"cell_type":"code","execution_count":null,"id":"cc0ed9a6","metadata":{"id":"cc0ed9a6","outputId":"e0e906bc-d4fd-4561-aef0-554ade7f2e83"},"outputs":[{"data":{"text/plain":["\"\\n\\narch = 'resnet18'\\nim_type = 'int' \\nn_slices = 130\\niq = 'fiq'\\niq_type = 'absolute'\\n\""]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["\n","arch = 'vgg8' #'resnet18' # 'vgg8'\n","im_type = 'int' #, 'rav', 'int_rav'\n","n_slices = 100 #130 for resnet18\n","iq = 'fiq' # 'all' #, 'fiq', 'viq', 'piq'])\n","iq_type = 'absolute' #, 'residual'])\n","'''\n","\n","arch = 'resnet18'\n","im_type = 'int'\n","n_slices = 130\n","iq = 'fiq'\n","iq_type = 'absolute'\n","'''"]},{"cell_type":"code","execution_count":null,"id":"7516a4fb","metadata":{"id":"7516a4fb"},"outputs":[],"source":["def setup_model(arch, no_of_slices, n_classes, device):\n","    if arch == 'resnet18':\n","        model = models.resnet18(num_classes=n_classes)\n","        # Set the number of input channels to 130\n","        #model.conv1 = nn.Conv2d(130, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        model.conv1 = nn.Conv2d(no_of_slices, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","    elif arch == 'vgg8':\n","        model = VGG8(in_channels=no_of_slices, num_classes=n_classes)\n","    else:\n","        raise Exception(f\"Invalid arch: {arch}\")\n","    model.double()\n","    model.to(device)\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"5eb7bc84","metadata":{"id":"5eb7bc84"},"outputs":[],"source":["def validate(model, val_loader, device):\n","    model.eval()\n","\n","    losses = []\n","    all_preds = []\n","\n","    with torch.no_grad():\n","        for (images, ages, _) in val_loader:\n","            # When batch_size=1 DataLoader doesn't convert the data to Tensors\n","            if not torch.is_tensor(images):\n","                images = torch.tensor(images).unsqueeze(0)\n","            if not torch.is_tensor(ages):\n","                ages = torch.tensor(ages).unsqueeze(0)\n","            images, ages = images.to(device), ages.to(device)\n","            age_preds = model(images).view(-1)\n","            loss = F.l1_loss(age_preds, ages, reduction='mean')\n","\n","            losses.append(loss.item())\n","            all_preds.extend(age_preds)\n","\n","    return np.mean(losses), torch.stack(all_preds).cpu().numpy()"]},{"cell_type":"code","execution_count":null,"id":"419eb6f7","metadata":{"id":"419eb6f7"},"outputs":[],"source":["def correct_path(path):\n","    path = path.replace('/ABIDE/', '/ABIDE_I/')\n","    path = path.replace('/NIH-PD/', '/NIH_PD/')\n","    if im_type == \"rav\":\n","        path = path.replace('/reg_', '/ravens_')\n","    return path\n","\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)\n","\n","checkpoint_dir = '/home/ch225256/iq_prediction/IQ_prediction/checkpoints/2729048'\n","\n","if iq == 'all':\n","    n_classes = 3\n","else:\n","    n_classes = 1\n","\n","data_list = '/home/ch225256/iq_prediction/IQ_prediction/folderlist2/all_iq_data_with_residuals_allok.list'\n","schema = {'id': str, 'fiq': float, 'fiq_r': float, 'piq': float, 'piq_r': float, 'viq': float, 'viq_r': float, 'sex': int, 'dg': int, 'age': float, 'path': str, 'site': int}\n","df = pd.read_csv(data_list, sep=' ', header=None, names=['id', 'fiq', 'fiq_r', 'piq', 'piq_r', 'viq', 'viq_r', 'sex', 'dg', 'age', 'path', 'site'], dtype=schema)\n","df['path'] = df['path'].apply(correct_path)\n","\n","kf5 = KFold(n_splits = 5, random_state = 1, shuffle = True)"]},{"cell_type":"code","execution_count":null,"id":"a0db9294","metadata":{"id":"a0db9294"},"outputs":[],"source":["# For producing validation results only\n","i = 1\n","for fold in kf5.split(df):\n","    validation = df.iloc[fold[1]]\n","    val_dataset = AgePredictionDataset(validation, iq=iq, iq_type=iq_type, n_slices=n_slices, im_type=im_type)\n","    val_loader = data.DataLoader(val_dataset, batch_size=1)\n","\n","    device = torch.device('cuda')\n","    model = setup_model(arch, n_slices, n_classes, device)\n","\n","    ## Training and evaluation\n","    if arch == 'resnet18':\n","        prefix = '2drn'\n","    else:\n","        prefix = '2dvgg'\n","\n","    model.load_state_dict(torch.load(f\"{checkpoint_dir}/{im_type}_{prefix}_{iq}_{n_slices}sl_{iq_type}_f{i}.pth\"))\n","    model.eval()\n","\n","    losses = []\n","    all_preds = []\n","\n","    with torch.no_grad():\n","        for (images, score, _) in val_loader:\n","            # When batch_size=1 DataLoader doesn't convert the data to Tensors\n","            if not torch.is_tensor(images):\n","                images = torch.tensor(images).unsqueeze(0)\n","            if not torch.is_tensor(score):\n","                score = torch.tensor(score).squeeze(-1)\n","            images, score = images.to(device), score.to(device)\n","            iq_preds = model(images).view(-1)\n","            loss = F.l1_loss(iq_preds, score, reduction='mean')\n","\n","            losses.append(loss.item())\n","            all_preds.extend(iq_preds)\n","\n","    print(f\"Loss in fold {i} is: {np.mean(losses)}\")\n","    i = i + 1\n","    break"]},{"cell_type":"code","execution_count":null,"id":"f93a8083","metadata":{"id":"f93a8083"},"outputs":[],"source":["# For selecting the layer for GradCAM\n","device = torch.device('cuda')\n","model = setup_model(arch, n_slices, n_classes, device)\n","## Training and evaluation\n","if arch == 'resnet18':\n","    prefix = '2drn'\n","else:\n","    prefix = '2dvgg'\n","\n","model.load_state_dict(torch.load(f\"{checkpoint_dir}/{im_type}_{prefix}_{iq}_{n_slices}sl_{iq_type}_f{i}.pth\"))\n","\n","if arch == 'resnet18':\n","    print(model.layer4[1].conv2)\n","else:\n","    print(model.conv41.conv1)"]},{"cell_type":"code","execution_count":null,"id":"bbcba721","metadata":{"id":"bbcba721"},"outputs":[],"source":["class InfoHolder():\n","\n","    def __init__(self, heatmap_layer):\n","        self.gradient = None\n","        self.activation = None\n","        self.heatmap_layer = heatmap_layer\n","\n","    def get_gradient(self, grad):\n","        self.gradient = grad\n","\n","    def hook(self, model, input, output):\n","        output.register_hook(self.get_gradient)\n","        self.activation = output.detach()\n","\n","def generate_heatmap(weighted_activation):\n","    raw_heatmap = torch.mean(weighted_activation, 0)\n","    heatmap = np.maximum(raw_heatmap.detach().cpu(), 0)\n","    heatmap /= torch.max(heatmap) + 1e-10\n","    return heatmap.numpy()\n","\n","def superimpose(input_img, heatmap):\n","    img = cv2.cvtColor(input_img,cv2.COLOR_BGR2RGB)\n","    heatmap = cv2.resize(heatmap, (img.shape[0], img.shape[1]))\n","    heatmap = np.uint8(255 * heatmap)\n","    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n","    heatmap = np.transpose(heatmap, (1, 0, 2))\n","\n","    superimposed_img = np.uint8(heatmap * 0.6 + img * 0.4)\n","    pil_img = cv2.cvtColor(superimposed_img,cv2.COLOR_BGR2RGB)\n","    return pil_img\n","\n","def to_RGB(tensor):\n","    tensor = (tensor - tensor.min())\n","    tensor = tensor/(tensor.max() + 1e-10)\n","    image_binary = np.transpose(tensor.cpu().detach().numpy(), (1, 2, 0))\n","    image = np.uint8(255 * image_binary)\n","    return image\n","\n","#image_binary = np.transpose(tensor.numpy(), (1, 2, 0))\n","\n","def grad_cam(model, input_tensor, heatmap_layer, truelabel=None):\n","    info = InfoHolder(heatmap_layer)\n","    heatmap_layer.register_forward_hook(info.hook)\n","\n","    output = model(input_tensor.unsqueeze(0))[0]\n","    truelabel = truelabel if truelabel else torch.argmax(output)\n","\n","    output[truelabel].backward()\n","\n","    weights = torch.mean(info.gradient, [0, 2, 3])\n","    activation = info.activation.squeeze(0)\n","\n","    weighted_activation = torch.zeros(activation.shape)\n","    for idx, (weight, activation) in enumerate(zip(weights, activation)):\n","        weighted_activation[idx] = weight * activation\n","\n","    heatmap = generate_heatmap(weighted_activation)\n","    input_image = to_RGB(input_tensor)\n","    #print(input_image.shape)\n","    input_image = input_image[:, :, 74:77]\n","    #print(input_image.shape)\n","    #input_image = np.transpose(input_image, (1, 2, 0))\n","    #print(input_image.shape)\n","    pil_img = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n","\n","    return pil_img, superimpose(input_image, heatmap)"]},{"cell_type":"code","execution_count":null,"id":"edf2ed7b","metadata":{"id":"edf2ed7b"},"outputs":[],"source":["# ResNet18\n","import matplotlib.pyplot as plt\n","f = 0\n","for fold in kf5.split(df):\n","    f += 1\n","    validation = df.iloc[fold[1]]\n","    val_dataset = AgePredictionDataset(validation, iq=iq, iq_type=iq_type, n_slices=n_slices, im_type=im_type)\n","    val_loader = data.DataLoader(val_dataset, batch_size=1)\n","\n","    device = torch.device('cuda')\n","    model = setup_model(arch, n_slices, n_classes, device)\n","\n","    ## Training and evaluation\n","    if arch == 'resnet18':\n","        prefix = '2drn'\n","    else:\n","        prefix = '2dvgg'\n","\n","    model.load_state_dict(torch.load(f\"{checkpoint_dir}/{im_type}_{prefix}_{iq}_{n_slices}sl_{iq_type}_f{f}.pth\"))\n","    model.eval()\n","\n","    losses = []\n","    all_preds = []\n","\n","    for (images, score, _) in val_loader:\n","        images, score = images.to(device), score.to(device)\n","        images = torch.squeeze(images)\n","\n","        heatmap_layer = model.layer4[1].conv2\n","        image, cam_image = grad_cam(model, images, heatmap_layer)\n","\n","        '''\n","        plt.figure\n","        plt.subplot(1, 2, 1)\n","        plt.imshow(image, cmap = 'gray')\n","        plt.subplot(1, 2, 2)\n","        plt.imshow(cam_combined)\n","        plt.show()\n","\n","\n","        plt.imshow(image)\n","        plt.savefig(f\"/home/ch225256/iq_prediction/IQ_prediction/age_prediction/images/im_{prefix}_{im_type}_{n_slices}_{iq}_{iq_type}_f{i}_{j}.png\")\n","\n","        plt.imshow(cam_image)\n","        plt.savefig(f\"/home/ch225256/iq_prediction/IQ_prediction/age_prediction/images/gradcam_{prefix}_{im_type}_{n_slices}_{iq}_{iq_type}_f{i}_{j}.png\")\n","        '''\n","        break\n","\n","    plt.figure(figsize=(15, 7))\n","    plt.subplot(1, 2, 1)\n","    plt.imshow(image, cmap = 'gray')\n","    plt.savefig(f\"/home/ch225256/iq_prediction/IQ_prediction/age_prediction/images/resnet18_image_{f}.png\")\n","    plt.subplot(1, 2, 2)\n","    plt.imshow(cam_image)\n","    plt.savefig(f\"/home/ch225256/iq_prediction/IQ_prediction/age_prediction/images/resnet18_cam_{f}.png\")\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"57a904a8-69d5-4a79-9aa1-b87254ecc6f6","metadata":{"id":"57a904a8-69d5-4a79-9aa1-b87254ecc6f6"},"outputs":[],"source":["# ResNet18 - GradCam Averaging\n","import matplotlib.pyplot as plt\n","f = 0\n","count = 0\n","for fold in kf5.split(df):\n","    f += 1\n","    validation = df.iloc[fold[1]]\n","    val_dataset = AgePredictionDataset(validation, iq=iq, iq_type=iq_type, n_slices=n_slices, im_type=im_type)\n","    val_loader = data.DataLoader(val_dataset, batch_size=1)\n","\n","    device = torch.device('cuda')\n","    model = setup_model(arch, n_slices, n_classes, device)\n","\n","    ## Training and evaluation\n","    if arch == 'resnet18':\n","        prefix = '2drn'\n","    else:\n","        prefix = '2dvgg'\n","\n","    model.load_state_dict(torch.load(f\"{checkpoint_dir}/{im_type}_{prefix}_{iq}_{n_slices}sl_{iq_type}_f{f}.pth\"))\n","    model.eval()\n","\n","    losses = []\n","    all_preds = []\n","\n","    for (images, score, _) in val_loader:\n","        images, score = images.to(device), score.to(device)\n","        images = torch.squeeze(images)\n","\n","        heatmap_layer = model.layer4[1].conv2\n","        image, cam_image = grad_cam(model, images, heatmap_layer)\n","\n","        if count == 0:\n","            cam_combined = cam_image\n","        else:\n","            cam_combined = cam_combined + cam_image\n","\n","        count += 1\n","\n","cam_combined2 = cam_combined/count\n","image[image > 0] = 1\n","cam_combined2 = np.multiply(cam_combined2, image)\n","\n","plt.figure\n","plt.subplot(1, 2, 1)\n","plt.imshow(image, cmap = 'gray')\n","plt.subplot(1, 2, 2)\n","plt.imshow(cam_combined2)\n","plt.show()\n","\n","'''\n","plt.figure\n","plt.subplot(1, 2, 1)\n","plt.imshow(image, cmap = 'gray')\n","plt.subplot(1, 2, 2)\n","plt.imshow(cam_combined)\n","plt.show()\n","\n","\n","plt.imshow(image)\n","plt.savefig(f\"/home/ch225256/iq_prediction/IQ_prediction/age_prediction/images/im_{prefix}_{im_type}_{n_slices}_{iq}_{iq_type}_f{i}_{j}.png\")\n","\n","plt.imshow(cam_image)\n","plt.savefig(f\"/home/ch225256/iq_prediction/IQ_prediction/age_prediction/images/gradcam_{prefix}_{im_type}_{n_slices}_{iq}_{iq_type}_f{i}_{j}.png\")\n","        '''"]},{"cell_type":"code","execution_count":null,"id":"9950a25d","metadata":{"id":"9950a25d"},"outputs":[],"source":["# VGG8\n","import matplotlib.pyplot as plt\n","f = 0\n","for fold in kf5.split(df):\n","    f += 1\n","    if f != 5:\n","        continue\n","    validation = df.iloc[fold[1]]\n","    val_dataset = AgePredictionDataset(validation, iq=iq, iq_type=iq_type, n_slices=n_slices, im_type=im_type)\n","    val_loader = data.DataLoader(val_dataset, batch_size=1)\n","\n","    device = torch.device('cuda')\n","    model = setup_model(arch, n_slices, n_classes, device)\n","\n","    ## Training and evaluation\n","    if arch == 'resnet18':\n","        prefix = '2drn'\n","    else:\n","        prefix = '2dvgg'\n","\n","    model.load_state_dict(torch.load(f\"{checkpoint_dir}/{im_type}_{prefix}_{iq}_{n_slices}sl_{iq_type}_f{f}.pth\"))\n","    model.eval()\n","\n","    losses = []\n","    all_preds = []\n","\n","    j = 1\n","    for (images, score, _) in val_loader:\n","        images, score = images.to(device), score.to(device)\n","        images = torch.squeeze(images)\n","\n","        heatmap_layer = model.conv41.conv1\n","        image, cam_image = grad_cam(model, images, heatmap_layer)\n","\n","\n","        plt.figure(figsize=(15, 7))\n","        plt.subplot(1, 2, 1)\n","        plt.imshow(image, cmap = 'gray')\n","        plt.savefig(f\"/home/ch225256/iq_prediction/IQ_prediction/age_prediction/images/vgg8_image_{j}.png\")\n","        plt.subplot(1, 2, 2)\n","        plt.imshow(cam_image)\n","        plt.savefig(f\"/home/ch225256/iq_prediction/IQ_prediction/age_prediction/images/vgg8_cam_{j}.png\")\n","        plt.show()\n","\n","        j += 1\n","\n","        '''\n","        plt.imshow(image)\n","        plt.savefig(f\"/home/ch225256/iq_prediction/IQ_prediction/age_prediction/images/im_{prefix}_{im_type}_{n_slices}_{iq}_{iq_type}_f{i}_{j}.png\")\n","\n","        plt.imshow(cam_image)\n","        plt.savefig(f\"/home/ch225256/iq_prediction/IQ_prediction/age_prediction/images/gradcam_{prefix}_{im_type}_{n_slices}_{iq}_{iq_type}_f{i}_{j}.png\")\n","        '''\n","        #break\n","\n","    '''\n","    plt.figure(figsize=(15, 7))\n","    plt.subplot(1, 2, 1)\n","    plt.imshow(image, cmap = 'gray')\n","    plt.savefig(f\"/home/ch225256/iq_prediction/IQ_prediction/age_prediction/images/vgg8_image_{f}.png\")\n","    plt.subplot(1, 2, 2)\n","    plt.imshow(cam_image)\n","    plt.savefig(f\"/home/ch225256/iq_prediction/IQ_prediction/age_prediction/images/vgg8_cam_{f}.png\")\n","    plt.show()\n","    '''"]},{"cell_type":"code","execution_count":null,"id":"507ea527-252c-4741-bf98-b49d4c75512d","metadata":{"id":"507ea527-252c-4741-bf98-b49d4c75512d"},"outputs":[],"source":["# VGG8 - Gradcam average\n","import matplotlib.pyplot as plt\n","f = 0\n","count = 0\n","for fold in kf5.split(df):\n","    f += 1\n","    if f != 5:\n","        continue\n","    validation = df.iloc[fold[1]]\n","    val_dataset = AgePredictionDataset(validation, iq=iq, iq_type=iq_type, n_slices=n_slices, im_type=im_type)\n","    val_loader = data.DataLoader(val_dataset, batch_size=1)\n","\n","    device = torch.device('cuda')\n","    model = setup_model(arch, n_slices, n_classes, device)\n","\n","    ## Training and evaluation\n","    if arch == 'resnet18':\n","        prefix = '2drn'\n","    else:\n","        prefix = '2dvgg'\n","\n","    model.load_state_dict(torch.load(f\"{checkpoint_dir}/{im_type}_{prefix}_{iq}_{n_slices}sl_{iq_type}_f{f}.pth\"))\n","    model.eval()\n","\n","    losses = []\n","    all_preds = []\n","\n","    for (images, score, _) in val_loader:\n","        images, score = images.to(device), score.to(device)\n","        images = torch.squeeze(images)\n","\n","        heatmap_layer = model.conv41.conv1\n","        image, cam_image = grad_cam(model, images, heatmap_layer)\n","\n","        if count == 0:\n","            cam_combined = cam_image\n","        else:\n","            cam_combined = cam_combined + cam_image\n","\n","        count += 1\n","\n","cam_combined2 = cam_combined/count\n","image[image > 0] = 1\n","cam_combined2 = np.multiply(cam_combined2, image)\n","\n","plt.figure\n","plt.subplot(1, 2, 1)\n","plt.imshow(image, cmap = 'gray')\n","plt.subplot(1, 2, 2)\n","plt.imshow(cam_combined2)\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}